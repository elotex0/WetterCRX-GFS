name: Fetch GFS GRIB2 and Generate PNGs

on:
  workflow_dispatch:

jobs:
  # -------------------------------------------------------
  # 1Ô∏è‚É£ Fetch all GRIB2 data
  # -------------------------------------------------------
  fetch_and_generate:
    runs-on: ubuntu-latest
    outputs:
      run: ${{ steps.set_run_date.outputs.run }}
      date: ${{ steps.set_run_date.outputs.date }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GFS_PAT }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Cache Python packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: pip install -r requirements.txt

      # 1) Cache versuchen zu holen
      - name: Restore wgrib2 cache
        id: cache-wgrib2
        uses: actions/cache@v4
        with:
          path: /usr/local/bin/wgrib2
          key: wgrib2-binary-v1

      # 2) Falls cache verpasst ‚Üí wgrib2 builden
      - name: Build wgrib2 from source
        if: steps.cache-wgrib2.outputs.cache-hit != 'true'
        run: |
          sudo apt-get update
          sudo apt-get install -y gfortran make wget
          wget https://ftp.cpc.ncep.noaa.gov/wd51we/wgrib2/wgrib2.tgz
          tar xvf wgrib2.tgz
          cd grib2
          make
          sudo cp wgrib2/wgrib2 /usr/local/bin/wgrib2

      # 3) Test ob alles funktioniert
      - name: Check wgrib2
        run: wgrib2 -version

  
      - name: Set RUN and DATE
        id: set_run_date
        run: |
          HOUR=$(date -u +%H)
          case $HOUR in
            05|06|07|08|09) RUN=00 ;;
            11|12|13|14) RUN=06 ;;
            17|18|19|20) RUN=12 ;;
            23|00|01|02) RUN=18 ;;
          esac
          DATE=$(date -u +%Y%m%d)
          echo "RUN=$RUN" >> $GITHUB_ENV
          echo "DATE=$DATE" >> $GITHUB_ENV
          echo "run=$RUN" >> $GITHUB_OUTPUT
          echo "date=$DATE" >> $GITHUB_OUTPUT

      - name: Download GFS GRIB2 data via Python filters
        run: |
          pip install requests
          export DATE=${{ env.DATE }}
          export RUN=${{ env.RUN }}
      
          echo "üîπ Starte Python-Downloads f√ºr GFS (DATE=$DATE, RUN=$RUN)"
          python scripts/download_pmsl.py
      
          echo "‚úÖ Alle Variablen wurden erfolgreich geladen!"


      - name: Upload GRIB2 as artifact
        uses: actions/upload-artifact@v4
        with:
          name: grib2
          path: data/

      - name: Delete GRIB2 files (local cleanup)
        run: rm -rf data/

  # -------------------------------------------------------
  # 2Ô∏è‚É£ Generate PNGs in parallel (matrix)
  # -------------------------------------------------------
  generate_pngs:
    runs-on: ubuntu-latest
    needs: fetch_and_generate
    strategy:
      matrix:
        variable: [pmsl, pmsl_eu]
      max-parallel: 2
      
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download GRIB2 artifact
        uses: actions/download-artifact@v4
        with:
          name: grib2
          path: data/

      - name: Generate PNGs for ${{ matrix.variable }}
        run: |
          mkdir -p iconeu/${{ matrix.variable }}
          input_dir="data/${{ matrix.variable }}"
          if [ "${{ matrix.variable }}" = "pmsl_eu" ]; then
            input_dir="data/pmsl"
          fi
          python scripts/generate_pngs.py \
            "$input_dir" \
            "gfs/${{ matrix.variable }}" \
            "${{ matrix.variable }}"


      - name: Upload PNGs artifact
        uses: actions/upload-artifact@v4
        with:
          name: gfs-${{ matrix.variable }}
          path: gfs/${{ matrix.variable }}

  # -------------------------------------------------------
  # 3Ô∏è‚É£ Merge PNGs + Deploy to R2
  # -------------------------------------------------------
  deploy_to_r2:
    runs-on: ubuntu-latest
    needs: [fetch_and_generate, generate_pngs]
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Download all PNG artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: gfs-*
          path: gfs_raw

      - name: Merge PNG folders into one structure
        run: |
          mkdir -p gfs/${{ needs.fetch_and_generate.outputs.run }}
          for d in gfs_raw/*; do
            if [ -d "$d" ]; then
              varname=$(basename "$d" | sed 's/^gfs-//')  # entfernt "gfs-" Prefix
              mkdir -p gfs/${{ needs.fetch_and_generate.outputs.run }}/"$varname"
              cp -r "$d"/* gfs/${{ needs.fetch_and_generate.outputs.run }}/"$varname"/ || true
            fi
          done


          echo "Merged structure:"
          ls -R gfs/${{ needs.fetch_and_generate.outputs.run }}

      - name: Generate Metadata
        run: |
          python scripts/generate_metadata.py \
            gfs/${{ needs.fetch_and_generate.outputs.run }} \
            ${{ needs.fetch_and_generate.outputs.run }} \
            ${{ needs.fetch_and_generate.outputs.date }}

      - name: Clean old runs on R2 except current
        run: |
          for run_folder in $(aws s3 ls s3://${{ secrets.R2_BUCKET }}/gfs/ \
            --endpoint-url https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com | awk '{print $2}' | sed 's#/##'); do
            if [ "$run_folder" != "${{ needs.fetch_and_generate.outputs.run }}/" ]; then
              aws s3 rm s3://${{ secrets.R2_BUCKET }}/gfs/$run_folder --recursive \
                --endpoint-url https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
            fi
          done
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Upload current run and metadata.json to R2
        run: |
          aws s3 sync ./gfs/${{ needs.fetch_and_generate.outputs.run }}/ \
            s3://${{ secrets.R2_BUCKET }}/gfs/${{ needs.fetch_and_generate.outputs.run }}/ \
            --endpoint-url https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com

          aws s3 cp ./gfs/metadata.json \
            s3://${{ secrets.R2_BUCKET }}/gfs/metadata.json \
            --endpoint-url https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
